{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==1.4.0 (from -r requirements.txt (line 1))\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: aiohttp==3.9.5 in c:\\users\\kgbad\\onedrive\\documents\\github\\homophobia-detection\\.conda\\lib\\site-packages (from -r requirements.txt (line 2)) (3.9.5)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in c:\\users\\kgbad\\onedrive\\documents\\github\\homophobia-detection\\.conda\\lib\\site-packages (from -r requirements.txt (line 3)) (1.3.1)\n",
      "Collecting alabaster==0.7.16 (from -r requirements.txt (line 4))\n",
      "  Using cached alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting albumentations==1.3.1 (from -r requirements.txt (line 5))\n",
      "  Using cached albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting altair==4.2.2 (from -r requirements.txt (line 6))\n",
      "  Using cached altair-4.2.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting annotated-types==0.7.0 (from -r requirements.txt (line 7))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anyio==3.7.1 (from -r requirements.txt (line 8))\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi==23.1.0 (from -r requirements.txt (line 9))\n",
      "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting argon2-cffi-bindings==21.2.0 (from -r requirements.txt (line 10))\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting array_record==0.4.1 (from -r requirements.txt (line 11))\n",
      "  Using cached array_record-0.4.1-py310-none-any.whl.metadata (503 bytes)\n",
      "Collecting arviz==0.15.1 (from -r requirements.txt (line 12))\n",
      "  Using cached arviz-0.15.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting astropy==5.3.4 (from -r requirements.txt (line 13))\n",
      "  Using cached astropy-5.3.4-cp311-cp311-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting astunparse==1.6.3 (from -r requirements.txt (line 14))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting async-timeout==4.0.3 (from -r requirements.txt (line 15))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting atpublic==4.1.0 (from -r requirements.txt (line 16))\n",
      "  Using cached atpublic-4.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: attrs==23.2.0 in c:\\users\\kgbad\\onedrive\\documents\\github\\homophobia-detection\\.conda\\lib\\site-packages (from -r requirements.txt (line 17)) (23.2.0)\n",
      "Collecting audioread==3.0.1 (from -r requirements.txt (line 18))\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting autograd==1.6.2 (from -r requirements.txt (line 19))\n",
      "  Using cached autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n",
      "Collecting Babel==2.15.0 (from -r requirements.txt (line 20))\n",
      "  Using cached Babel-2.15.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backcall==0.2.0 in c:\\users\\kgbad\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 21)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 22))\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bidict==0.23.1 (from -r requirements.txt (line 23))\n",
      "  Using cached bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting bigframes==1.8.0 (from -r requirements.txt (line 24))\n",
      "  Using cached bigframes-1.8.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting bleach==6.1.0 (from -r requirements.txt (line 25))\n",
      "  Using cached bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting blinker==1.4 (from -r requirements.txt (line 26))\n",
      "  Using cached blinker-1.4.tar.gz (111 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting blis==0.7.11 (from -r requirements.txt (line 27))\n",
      "  Using cached blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting blosc2==2.0.0 (from -r requirements.txt (line 28))\n",
      "  Using cached blosc2-2.0.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting bokeh==3.3.4 (from -r requirements.txt (line 29))\n",
      "  Using cached bokeh-3.3.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting bqplot==0.12.43 (from -r requirements.txt (line 30))\n",
      "  Using cached bqplot-0.12.43-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting branca==0.7.2 (from -r requirements.txt (line 31))\n",
      "  Using cached branca-0.7.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting build==1.2.1 (from -r requirements.txt (line 32))\n",
      "  Using cached build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting CacheControl==0.14.0 (from -r requirements.txt (line 33))\n",
      "  Using cached cachecontrol-0.14.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting cachetools==5.3.3 (from -r requirements.txt (line 34))\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting catalogue==2.0.10 (from -r requirements.txt (line 35))\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: certifi==2024.6.2 in c:\\users\\kgbad\\onedrive\\documents\\github\\homophobia-detection\\.conda\\lib\\site-packages (from -r requirements.txt (line 36)) (2024.6.2)\n",
      "Collecting cffi==1.16.0 (from -r requirements.txt (line 37))\n",
      "  Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting chardet==5.2.0 (from -r requirements.txt (line 38))\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in c:\\users\\kgbad\\onedrive\\documents\\github\\homophobia-detection\\.conda\\lib\\site-packages (from -r requirements.txt (line 39)) (3.3.2)\n",
      "Collecting chex==0.1.86 (from -r requirements.txt (line 40))\n",
      "  Using cached chex-0.1.86-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting click==8.1.7 (from -r requirements.txt (line 41))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting click-plugins==1.1.1 (from -r requirements.txt (line 42))\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting cligj==0.7.2 (from -r requirements.txt (line 43))\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting cloudpathlib==0.18.1 (from -r requirements.txt (line 44))\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cloudpickle==2.2.1 (from -r requirements.txt (line 45))\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting cmake==3.27.9 (from -r requirements.txt (line 46))\n",
      "  Using cached cmake-3.27.9-py2.py3-none-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting cmdstanpy==1.2.3 (from -r requirements.txt (line 47))\n",
      "  Using cached cmdstanpy-1.2.3-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting colorcet==3.1.0 (from -r requirements.txt (line 48))\n",
      "  Using cached colorcet-3.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting colorlover==0.3.0 (from -r requirements.txt (line 49))\n",
      "  Using cached colorlover-0.3.0-py3-none-any.whl.metadata (421 bytes)\n",
      "Collecting colour==0.1.5 (from -r requirements.txt (line 50))\n",
      "  Using cached colour-0.1.5-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting community==1.0.0b1 (from -r requirements.txt (line 51))\n",
      "  Using cached community-1.0.0b1.tar.gz (2.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting confection==0.1.5 (from -r requirements.txt (line 52))\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cons==0.4.6 (from -r requirements.txt (line 53))\n",
      "  Using cached cons-0.4.6.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting contextlib2==21.6.0 (from -r requirements.txt (line 54))\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting contourpy==1.2.1 (from -r requirements.txt (line 55))\n",
      "  Using cached contourpy-1.2.1-cp311-cp311-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cryptography==42.0.8 (from -r requirements.txt (line 56))\n",
      "  Using cached cryptography-42.0.8-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cuda-python==12.2.1 (from -r requirements.txt (line 57))\n",
      "  Using cached cuda_python-12.2.1-cp311-cp311-win_amd64.whl.metadata (770 bytes)\n",
      "Collecting cudf-cu12==24.4.1 (from -r requirements.txt (line 58))\n",
      "  Using cached cudf_cu12-24.4.1.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [55 lines of output]\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\wheel.py\", line 147, in download_wheel\n",
      "          return download_manual(wheel_directory, distribution, version)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\wheel.py\", line 114, in download_manual\n",
      "          raise RuntimeError(f\"Didn't find wheel for {distribution} {version}\")\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\wheel.py\", line 147, in download_wheel\n",
      "          return download_manual(wheel_directory, distribution, version)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\wheel.py\", line 114, in download_manual\n",
      "          raise RuntimeError(f\"Didn't find wheel for {distribution} {version}\")\n",
      "      RuntimeError: Didn't find wheel for cudf-cu12 24.4.1\n",
      "      \n",
      "      During handling of the above exception, another exception occurred:\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\kgbad\\OneDrive\\Documents\\GitHub\\homophobia-detection\\.conda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\kgbad\\OneDrive\\Documents\\GitHub\\homophobia-detection\\.conda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\kgbad\\OneDrive\\Documents\\GitHub\\homophobia-detection\\.conda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 152, in prepare_metadata_for_build_wheel\n",
      "          whl_basename = backend.build_wheel(metadata_directory, config_settings)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\buildapi.py\", line 29, in build_wheel\n",
      "          return download_wheel(pathlib.Path(wheel_directory), config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\wheel.py\", line 149, in download_wheel\n",
      "          report_install_failure(distribution, version, exception_context)\n",
      "        File \"C:\\Users\\kgbad\\AppData\\Local\\Temp\\pip-build-env-yg17msa2\\overlay\\Lib\\site-packages\\nvidia_stub\\error.py\", line 63, in report_install_failure\n",
      "          raise InstallFailedError(\n",
      "      nvidia_stub.error.InstallFailedError:\n",
      "      *******************************************************************************\n",
      "      \n",
      "      The installation of cudf-cu12 for version 24.4.1 failed.\n",
      "      \n",
      "      This is a special placeholder package which downloads a real wheel package\n",
      "      from https://pypi.nvidia.com. If https://pypi.nvidia.com is not reachable, we\n",
      "      cannot download the real wheel file to install.\n",
      "      \n",
      "      You might try installing this package via\n",
      "      ```\n",
      "      $ pip install --extra-index-url https://pypi.nvidia.com cudf-cu12\n",
      "      ```\n",
      "      \n",
      "      Here is some debug information about your platform to include in any bug\n",
      "      report:\n",
      "      \n",
      "      Python Version: CPython 3.11.9\n",
      "      Operating System: Windows 10\n",
      "      CPU Architecture: AMD64\n",
      "      nvidia-smi command not found. Ensure NVIDIA drivers are installed.\n",
      "      \n",
      "      *******************************************************************************\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "#model2 = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"my_model_hate\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kgbad\\OneDrive\\Documents\\GitHub\\homophobia-detection\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kgbad\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\kgbad\\OneDrive\\Documents\\GitHub\\homophobia-detection\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_phrase(phrase):\n",
    "    inputs = tokenizer(phrase, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return predicted_class_id\n",
    "\n",
    "phrase = \"i hate lesbians\"\n",
    "predicted_label = predict_phrase(phrase)\n",
    "predicted_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
